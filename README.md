# SA√â 302 : Int√©gration de donn√©es dans un Data Warehouse (Formule 1)

**Ann√©e :** 2025-2026  
**√âtablissement :** IUT Lumi√®re Lyon 2  
**D√©partement :** Science des Donn√©es

## √âquipe Projet
* **MARRE Ewann**
* **MAURIN Antoine**
* **SANZ Rafa√´l**
* **ZAVAGNO Quentin**


## 1. Pr√©sentation du Projet
Ce projet s'inscrit dans le cadre de la SA√â 302. L'objectif est de concevoir et mettre en ≈ìuvre un pipeline d√©cisionnel complet (BI) ‚Äî de l'extraction des donn√©es brutes jusqu'√† la visualisation ‚Äî appliqu√© √† l'historique du championnat du monde de **Formule 1**.

Nous avons transform√© des fichiers plats h√©t√©rog√®nes en un syst√®me d'information d√©cisionnel performant permettant d'analyser plus de 70 ans de courses.


## 2. Objectifs D√©cisionnels
L'analyse vise √† r√©pondre aux questions cl√©s que les fans ou les analystes sportifs se posent :
* **Performance Historique :** Quels sont les pilotes et constructeurs les plus titr√©s de l'histoire ?
* **√âvolution Technologique :** Comment les temps au tour et les vitesses ont-ils √©volu√© sur un m√™me circuit au fil des d√©cennies ?
* **Fiabilit√© :** Quel est le taux d'abandon (panne moteur, accident) par constructeur ?
* **Analyse de Carri√®re :** Quel pilote a la meilleure position d'arriv√©e moyenne ?
* **G√©ographie :** R√©partition mondiale des Grands Prix.


## 3. Architecture et Pipeline BI
Le projet suit une architecture en couches classiques, orchestr√©e par **Pentaho Data Integration (PDI)** :

1.  **Sources de Donn√©es (CSV) :**
    * Donn√©es brutes issues de Kaggle (Rohan Rao Dataset).
    * Contenu : Pilotes, r√©sultats, temps au tour, arr√™ts aux stands, qualifications, etc.
    * *Volume :* ~14 tables couvrant la p√©riode 1950-2023.

2.  **Zone RAW (PostgreSQL) :**
    * Ingestion des fichiers CSV "tels quels" dans le sch√©ma `raw`.
    * Script : `job_raw.kjb`.

3.  **Zone SAS - Storage Area System (PostgreSQL) :**
    * Nettoyage et typage des donn√©es.
    * Traitement des valeurs nulles (gestion du caract√®re `\N`).
    * Harmonisation des formats de dates et des noms de pays.
    * Script : `job_sas.kjb`.

4.  **Data Warehouse (PostgreSQL) :**
    * Stockage structur√© en **Sch√©ma en √âtoile** dans le sch√©ma `dwh`.
    * G√©n√©ration des cl√©s techniques (Surrogate Keys).

5.  **Reporting (Power BI) :**
    * Tableau de bord interactif (`PB_F1.pbix`) connect√© au DWH.


## 4. Mod√©lisation (Sch√©ma en √âtoile)
Afin d'optimiser les performances des requ√™tes analytiques, nous avons mod√©lis√© les donn√©es comme suit :

### Tables de Faits
* **`fact_results`** : Table centrale (granularit√© : pilote/course). Contient les points, le rang, le temps de course en millisecondes et la vitesse.
* **`fact_lap_times`** : D√©tail des temps au tour pour des analyses de performance fine.

### Dimensions
* **`dim_drivers`** : Informations sur les pilotes (Nom, nationalit√©, date de naissance).
* **`dim_constructeurs`** : √âcuries (Ferrari, McLaren, etc.) et nationalit√©s.
* **`dim_circuits`** : G√©olocalisation (Lat/Long), altitude et pays.
* **`dim_races`** : Informations sur l'√©v√©nement (Nom du GP, ann√©e).
* **`dim_saisons`** & **`dim_calendrier`** : Axe temporel.
* **`dim_status`** : R√©f√©rentiel des causes de fin de course (Fini, Accident, Panne...).


## 5. Installation et Utilisation

### Pr√©requis
* **SGBD :** PostgreSQL
* **ETL :** Pentaho Data Integration (Spoon) 9.x ou sup√©rieur
* **Dataviz :** Microsoft Power BI Desktop

### Instructions de d√©ploiement

1.  **Clonage du d√©p√¥t :**
    ```bash
    git clone [https://github.com/votre-repo/dwh_marre_maurin_sanz_zavagno_5.git](https://github.com/votre-repo/dwh_marre_maurin_sanz_zavagno_5.git)
    ```

2.  **Pr√©paration de la Base de Donn√©es :**
    * Ex√©cuter les scripts SQL situ√©s dans `dataframe/DATABASES/` pour cr√©er les sch√©mas et les tables :
        1.  `raw_f1.sql`
        2.  `sas_f1.sql`
        3.  `dwh_f1.sql`

3.  **Configuration de l'ETL :**
    * Ouvrir Pentaho (Spoon).
    * Configurer la connexion √† votre base PostgreSQL (`db_f1`) dans le fichier `database.properties` ou directement dans les connexions du projet.
    * Mettre √† jour les chemins d'acc√®s aux fichiers CSV dans les transformations du dossier `_SOURCES`.

4.  **Ex√©cution des flux :**
    Lancer les Jobs dans l'ordre suivant :
    * `_TRANSFORMATIONS/job_raw.kjb` (Chargement des CSV)
    * `_TRANSFORMATIONS/job_sas.kjb` (Nettoyage)
    * Ex√©cuter ensuite les transformations de chargement du DWH (Dimensions puis Faits).

5.  **Visualisation :**
    * Ouvrir le fichier `PB_F1.pbix`.
    * Actualiser les donn√©es (n√©cessite de pointer vers votre instance locale PostgreSQL).


## 6. Structure du projet

```text
üì¶ dwh_marre_maurin_sanz_zavagno_5
 ‚î£ üìÇ dataframe
 ‚îÉ ‚î£ üìÇ DATABASES          # Scripts SQL de cr√©ation des tables (DDL)
 ‚îÉ ‚î£ üìÇ _SOURCES           # Fichiers CSV bruts
 ‚îÉ ‚îó üìÇ _TRANSFORMATIONS   # Flux ETL Pentaho (.ktr, .kjb)
 ‚îÉ   ‚î£ üìÇ _RAW             # Injections brutes
 ‚îÉ   ‚î£ üìÇ T_SAS            # Transformations de nettoyage
 ‚îÉ   ‚î£ üìÇ T_DIM            # Chargement des dimensions
 ‚îÉ   ‚îó üìÇ T_FACT           # Chargement des faits
 ‚î£ üìú PB_F1.pbix           # Rapport Power BI
 ‚îó üìú README.md
```


## üîó Liens utiles
* [Lien du tableau de bord](https://github.com/antoinemrn8/DWH_MARRE_MAURIN_SANZ_ZAVAGNO_5/blob/main/PB_F1.pbix)
* [Lien du rapport](https://github.com/antoinemrn8/DWH_MARRE_MAURIN_SANZ_ZAVAGNO_5/blob/main/DWH-MARRE-MAURIN-SANZ-ZAVAGNO-5.pdf)
* [Lien vers les transformations](https://github.com/antoinemrn8/DWH_MARRE_MAURIN_SANZ_ZAVAGNO_5/tree/main/dataframe/_TRANSFORMATIONS)
