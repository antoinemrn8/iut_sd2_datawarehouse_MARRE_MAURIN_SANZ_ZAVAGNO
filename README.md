# SAÃ‰ 302 : IntÃ©gration de donnÃ©es dans un Data Warehouse (Formule 1)

**AnnÃ©e :** 2025-2026  
**Ã‰tablissement :** IUT LumiÃ¨re Lyon 2  
**DÃ©partement :** Science des DonnÃ©es

## ğŸ‘¥ Ã‰quipe Projet
* **MARRE Ewann**
* **MAURIN Antoine**
* **SANZ RafaÃ«l**
* **ZAVAGNO Quentin**

---

## ğŸï¸ PrÃ©sentation du Projet
Ce projet s'inscrit dans le cadre de la SAÃ‰ 302. L'objectif est de concevoir et mettre en Å“uvre un pipeline dÃ©cisionnel complet (BI) â€” de l'extraction des donnÃ©es brutes jusqu'Ã  la visualisation â€” appliquÃ© Ã  l'historique du championnat du monde de **Formule 1**.

Nous avons transformÃ© des fichiers plats hÃ©tÃ©rogÃ¨nes en un systÃ¨me d'information dÃ©cisionnel performant permettant d'analyser plus de 70 ans de courses.

## ğŸ¯ Objectifs DÃ©cisionnels
L'analyse vise Ã  rÃ©pondre aux questions clÃ©s que les fans ou les analystes sportifs se posent :
* **Performance Historique :** Quels sont les pilotes et constructeurs les plus titrÃ©s de l'histoire ?
* **Ã‰volution Technologique :** Comment les temps au tour et les vitesses ont-ils Ã©voluÃ© sur un mÃªme circuit au fil des dÃ©cennies ?
* **FiabilitÃ© :** Quel est le taux d'abandon (panne moteur, accident) par constructeur ?
* **Analyse de CarriÃ¨re :** Quel pilote a la meilleure position d'arrivÃ©e moyenne ?
* **GÃ©ographie :** RÃ©partition mondiale des Grands Prix.

---

## ğŸ“‚ Architecture et Pipeline BI
Le projet suit une architecture en couches classiques, orchestrÃ©e par **Pentaho Data Integration (PDI)** :

1.  **Sources de DonnÃ©es (CSV) :**
    * DonnÃ©es brutes issues de Kaggle (Rohan Rao Dataset).
    * Contenu : Pilotes, rÃ©sultats, temps au tour, arrÃªts aux stands, qualifications, etc.
    * *Volume :* ~14 tables couvrant la pÃ©riode 1950-2023.

2.  **Zone RAW (PostgreSQL) :**
    * Ingestion des fichiers CSV "tels quels" dans le schÃ©ma `raw`.
    * Script : `job_raw.kjb`.

3.  **Zone SAS - Storage Area System (PostgreSQL) :**
    * Nettoyage et typage des donnÃ©es.
    * Traitement des valeurs nulles (gestion du caractÃ¨re `\N`).
    * Harmonisation des formats de dates et des noms de pays.
    * Script : `job_sas.kjb`.

4.  **Data Warehouse (PostgreSQL) :**
    * Stockage structurÃ© en **SchÃ©ma en Ã‰toile** dans le schÃ©ma `dwh`.
    * GÃ©nÃ©ration des clÃ©s techniques (Surrogate Keys).

5.  **Reporting (Power BI) :**
    * Tableau de bord interactif (`PB_F1.pbix`) connectÃ© au DWH.

---

## ğŸ“ ModÃ©lisation (SchÃ©ma en Ã‰toile)
Afin d'optimiser les performances des requÃªtes analytiques, nous avons modÃ©lisÃ© les donnÃ©es comme suit :

### Tables de Faits
* **`fact_results`** : Table centrale (granularitÃ© : pilote/course). Contient les points, le rang, le temps de course en millisecondes et la vitesse.
* **`fact_lap_times`** : DÃ©tail des temps au tour pour des analyses de performance fine.

### Dimensions
* **`dim_drivers`** : Informations sur les pilotes (Nom, nationalitÃ©, date de naissance).
* **`dim_constructeurs`** : Ã‰curies (Ferrari, McLaren, etc.) et nationalitÃ©s.
* **`dim_circuits`** : GÃ©olocalisation (Lat/Long), altitude et pays.
* **`dim_races`** : Informations sur l'Ã©vÃ©nement (Nom du GP, annÃ©e).
* **`dim_saisons`** & **`dim_calendrier`** : Axe temporel.
* **`dim_status`** : RÃ©fÃ©rentiel des causes de fin de course (Fini, Accident, Panne...).

---

## ğŸ› ï¸ Installation et Utilisation

### PrÃ©requis
* **SGBD :** PostgreSQL
* **ETL :** Pentaho Data Integration (Spoon) 9.x ou supÃ©rieur
* **Dataviz :** Microsoft Power BI Desktop

### Instructions de dÃ©ploiement

1.  **Clonage du dÃ©pÃ´t :**
    ```bash
    git clone [https://github.com/votre-repo/dwh_marre_maurin_sanz_zavagno_5.git](https://github.com/votre-repo/dwh_marre_maurin_sanz_zavagno_5.git)
    ```

2.  **PrÃ©paration de la Base de DonnÃ©es :**
    * ExÃ©cuter les scripts SQL situÃ©s dans `dataframe/DATABASES/` pour crÃ©er les schÃ©mas et les tables :
        1.  `raw_f1.sql`
        2.  `sas_f1.sql`
        3.  `dwh_f1.sql`

3.  **Configuration de l'ETL :**
    * Ouvrir Pentaho (Spoon).
    * Configurer la connexion Ã  votre base PostgreSQL (`db_f1`) dans le fichier `database.properties` ou directement dans les connexions du projet.
    * Mettre Ã  jour les chemins d'accÃ¨s aux fichiers CSV dans les transformations du dossier `_SOURCES`.

4.  **ExÃ©cution des flux :**
    Lancer les Jobs dans l'ordre suivant :
    * `_TRANSFORMATIONS/job_raw.kjb` (Chargement des CSV)
    * `_TRANSFORMATIONS/job_sas.kjb` (Nettoyage)
    * ExÃ©cuter ensuite les transformations de chargement du DWH (Dimensions puis Faits).

5.  **Visualisation :**
    * Ouvrir le fichier `PB_F1.pbix`.
    * Actualiser les donnÃ©es (nÃ©cessite de pointer vers votre instance locale PostgreSQL).

---

## ğŸ“‚ Structure du projet

```text
ğŸ“¦ dwh_marre_maurin_sanz_zavagno_5
 â”£ ğŸ“‚ dataframe
 â”ƒ â”£ ğŸ“‚ DATABASES          # Scripts SQL de crÃ©ation des tables (DDL)
 â”ƒ â”£ ğŸ“‚ _SOURCES           # Fichiers CSV bruts
 â”ƒ â”— ğŸ“‚ _TRANSFORMATIONS   # Flux ETL Pentaho (.ktr, .kjb)
 â”ƒ   â”£ ğŸ“‚ _RAW             # Injections brutes
 â”ƒ   â”£ ğŸ“‚ T_SAS            # Transformations de nettoyage
 â”ƒ   â”£ ğŸ“‚ T_DIM            # Chargement des dimensions
 â”ƒ   â”— ğŸ“‚ T_FACT           # Chargement des faits
 â”£ ğŸ“œ PB_F1.pbix           # Rapport Power BI
 â”— ğŸ“œ README.md
```
## ğŸ”— Liens utiles
* [Lien du tableau de bord](https://github.com/antoinemrn8/DWH_MARRE_MAURIN_SANZ_ZAVAGNO_5/blob/main/PB_F1.pbix)
* [Lien du rapport](https://github.com/antoinemrn8/DWH_MARRE_MAURIN_SANZ_ZAVAGNO_5/blob/main/DWH-MARRE-MAURIN-SANZ-ZAVAGNO-5.pdf)
* [Lien vers les transformations](https://github.com/antoinemrn8/DWH_MARRE_MAURIN_SANZ_ZAVAGNO_5/tree/main/dataframe/_TRANSFORMATIONS)
